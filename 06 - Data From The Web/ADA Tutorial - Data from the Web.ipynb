{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from the Web -  ADA 2018 Tutorial \n",
    "\n",
    "#### What do you find in this Notebook?\n",
    "\n",
    "The purpose of the Notebook is to offer a **quick** overview on how to scrape a Web page. In details, we illustrate the two main libraries used for this purpose. Afterwords, we show how to retrieve data from the Web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch data from the Web with Python, you need to get use to two essential libraries:\n",
    "\n",
    " * [`Requests (HTTP)`](http://docs.python-requests.org/en/master/user/quickstart/): get the `html` page to parse.\n",
    "\n",
    " * [`Beautiful Soup (HTML Parsing)`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): parse the `html` and extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a `get` request\n",
    "\n",
    "We start scraping this website: https://httpbin.org/ - HTTP Request & Response Service\n",
    "\n",
    "The website offers some useful endpoints to check the content of our request. Some of them provides an 'echo service' that reply with the request received."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Here we show an example on how use a get request. In particular, you see that we can get different information about the response:\n",
    "\n",
    "* The status code which tells us whether everything is fine and if the request worked\n",
    "* The headers\n",
    "* Body of the response (typically HTML for webpages or JSON/XML for web services)\n",
    "\n",
    "**NOTE:** this is an echo service, what you see is what we sent to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the request\n",
    "r = requests.get('https://httpbin.org/ip')\n",
    "\n",
    "print('Response status code: {0}\\n'.format(r.status_code))\n",
    "print('Response headers: {0}\\n'.format(r.headers))\n",
    "print('Response body: {0}'.format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the body of the response is a JSON string, Requests offers a convenient way to parse the text and get a Python dictionary.\n",
    "\n",
    "Let's try to get the current time from here: [https://now.httpbin.org/](https://now.httpbin.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://now.httpbin.org/')\n",
    "print('Response body (parsed json):')\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the `url` has been slightly changed to include a parameter (key1).\n",
    "\n",
    "Remember that the with the GET method the parameters are part of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://httpbin.org/get?key1=value1')\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a `post` request\n",
    "\n",
    "A POST request can have the paramenters in the body. Let's how to do this with Requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.post('https://httpbin.org/post', data=payload)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a request and extract the Page Title!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do the request and get the `html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the request\n",
    "r = requests.get('https://httpbin.org/html')\n",
    "r.text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thus, we start to use our beloved `BeautifulSoup` to parse the HTML and we get the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the header\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "soup.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get interesting data - DBLP\n",
    "\n",
    "*DBLP is a computer science bibliography website. Starting in 1993 at the University of Trier, Germany, it grew from a small collection of HTML files and became an organization hosting a database and logic programming bibliography site. DBLP listed more than 3.66 million journal articles, conference papers, and other publications on computer science in July 2016, up from about 14,000 in 1995.*\n",
    "\n",
    "<div align=\"right\">https://en.wikipedia.org/wiki/DBLP</div> \n",
    "\n",
    "We want to check the distribution of the publications by year of the president of EPFL - Martin Vetterli.\n",
    "\n",
    "First of all, let's check the page with the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://dblp.uni-trier.de/pers/hd/v/Vetterli:Martin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page is public and accessible with a browser using a simple GET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)\n",
    "page_body = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the page content is downloaded and we can inspect the body of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_body[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pure HTML, and we need BeautifulSoup to parse the content. We can specify the parser we want to use html.parser, lxml, lxml-xml, xml, html5lib. Each of them has advantages and disadvantages - see [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_body, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the page is parsed and we can read the data we need!\n",
    "\n",
    "For example, let's get the title! Are we in the right page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! And we can get the clean text without HTML tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex query now! Let's find all the links in the page. \n",
    "\n",
    "In HTML a link is defined using the tag &lt;A&gt;, and BeautifulSoup offers an easy way to find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all('a')\n",
    "print('The webpage cointains {0} links...'.format(len(all_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_links = 0\n",
    "for link in all_links:\n",
    "    if(not link.get('href').startswith('http://dblp.uni-trier.de/')\n",
    "       and link.get('href').startswith('http')):  # just an example, you need more checks\n",
    "        external_links += 1\n",
    "\n",
    "print('... and {0} of them point to external websites.'.format(external_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on. Now we want to extract the sections that contain the publication details.\n",
    "\n",
    "**The easiest way is to inspect the DOM of the web page with a browser.** Check with your browser how to isolate the portions of the page that represent publications. - Task not in this Notebook -\n",
    "\n",
    "Ok, each row is composed by a &lt;li&gt; tag and has a class called 'entry':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_wrappers = soup.find_all('li', class_='entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of items: {0}'.format(len(publications_wrappers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in publications_wrappers:\n",
    "    print(p.find('span', class_='title').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_list = []\n",
    "for p in publications_wrappers:\n",
    "    title = p.find('span', class_='title').text  # get the title\n",
    "    authos_list = p.find_all('span', {'itemprop': 'author'})  # get the authors list\n",
    "    authors = [author.text for author in authos_list]  \n",
    "    year = p.find('span', {'itemprop': 'datePublished'}).text\n",
    "    publications_list.append({'title': title, \n",
    "                         'authors': authors, \n",
    "                         'year': int(year)})  # here you should validate the data\n",
    "\n",
    "publications = pd.DataFrame.from_dict(publications_list)\n",
    "publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications.groupby('year')\\\n",
    "    .count()\\\n",
    "    .rename(columns = {'title':'count'})\\\n",
    "    .plot(y='count', kind='bar', grid=True, figsize=(10, 6), title='Data from: ' + URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
